# ==============================================================================
# Model and Trust Settings
# ==============================================================================
# Path to the base model's local directory. 
# This model will be loaded and used as the starting point for finetuning.
base_model: pretrained_models/Spark-TTS-0.5B/LLM
# Optional: Specify a Hugging Face Hub model ID for automatic upload after training.
hub_model_id: qhuy242/Vi-Spark-TTS-0.5B-Lora-VNAVC

# Allows loading custom code from the model's repository. Essential for custom architectures.
trust_remote_code: true
# Ensures strict parameter matching when loading the model. Setting to false is
# useful if the base model has unexpected parameters (e.g., from an older version).
strict: false

# ==============================================================================
# Dataset Configuration
# ==============================================================================
# Define the dataset to be used for training.
datasets:
  - path: json
    # Path to the JSONL file containing the tokenized data.
    data_files: ["output_prompt/tokenized_vnavc.jsonl"]
    # Specifies the dataset format. 'completion' means the model learns to
    # complete the provided text.
    type: completion
    
# Path to a pre-prepared dataset if it's already tokenized and formatted.
dataset_prepared_path:
# Percentage of the training data to use as a validation set.
val_set_size: 0.05
# Directory where the trained model and checkpoints will be saved.
output_dir: ./outputs/out


# ==============================================================================
# LoRA/QLoRA Settings - Memory and Performance Optimization
# ==============================================================================
# Activates and configures LoRA for efficient finetuning.
adapter: lora
load_in_8bit: false
load_in_4bit: false

lora_r: 32              # Increased from 16 for better capacity
lora_alpha: 64          # Increased proportionally (2x lora_r)
lora_dropout: 0.1       # Slightly increased for better regularization
lora_target_linear: true # Automatically targets all linear layers
lora_target_modules:
  - q_proj
  - v_proj
  - k_proj
  - o_proj
  - gate_proj           # Added for better coverage
  - up_proj             # Added for better coverage
  - down_proj           # Added for better coverage


# ==============================================================================
# Training Parameters
# ==============================================================================
# Maximum sequence length for padding and truncation.
sequence_len: 2048
# Enables packing multiple short sequences into a single sequence for efficiency.
sample_packing: true
eval_sample_packing: true
# Pads sequences to the `sequence_len` for consistent batch processing.
pad_to_sequence_len: true

# ==============================================================================
# Experiment Tracking with Weights & Biases
# ==============================================================================
# Configuration for Weights & Biases. Uncomment and fill to enable.
wandb_project: spark-tts-finetuning
wandb_entity: your_wandb_entity
wandb_watch: gradients
wandb_name: spark-tts-0.5b-lora_2epochs
wandb_log_model: checkpoint_and_end
wandb_run_id: 


# ==============================================================================
# Optimization Settings
# ==============================================================================
# Number of steps to accumulate gradients before performing a parameter update.
# Effective batch size = micro_batch_size * gradient_accumulation_steps * num_gpus.
gradient_accumulation_steps: 8
micro_batch_size: 2
# Number of training epochs.
num_epochs: 10
# AdamW optimizer, optimized for PyTorch on GPUs.
optimizer: adamw_torch_fused
# Learning rate scheduler. 
lr_scheduler: cosine_with_restarts
# L2 regularization to prevent overfitting.
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.999                    # Standard for full fine-tuning
adam_epsilon: 1e-8
# Learning rate for LoRA is typically higher than for full finetuning.
learning_rate: 5e-4

# Limit the total number of checkpoints to save. Saves storage space.
save_total_limit: 5 
# Loads the best-performing model (based on validation loss) at the end of training.
load_best_model_at_end: true 
# Number of epochs to wait for validation loss to improve before stopping training.
early_stopping_patience: 2

# Specifies whether to calculate loss on the input tokens as well.
# Setting to false focuses on generating the output tokens.
train_on_inputs: false
# Groups samples of similar lengths into the same batch to minimize padding.
group_by_length: false
# Automatically uses bfloat16 if the hardware supports it.
bf16: auto
# Explicitly set fp16 precision (if bf16 is not supported).
fp16:
# Enables tensor core acceleration for matrix multiplications.
tf32: true


# ==============================================================================
# Performance and Memory Optimizations
# ==============================================================================
# Enables gradient checkpointing to trade computation for memory, reducing VRAM.
gradient_checkpointing: true
# Configuration for gradient checkpointing.
gradient_checkpointing_kwargs:
  use_reentrant: false
# Path to a checkpoint to resume training from.
resume_from_checkpoint:
# Local rank for distributed training (automatically handled by `accelerate`).
local_rank:
# Number of steps after which to log training metrics.
logging_steps: 50
# Legacy attention implementation. Prefer `flash_attention`.
xformers_attention:
# Enables Flash Attention for faster and more memory-efficient attention.
flash_attention: true

# ==============================================================================
# Schedule and Debugging
# ==============================================================================
# Defines the proportion of total steps to use for a warmup learning rate.
warmup_ratio: 0.03
# Number of evaluation runs per epoch.
evals_per_epoch: 1
# Number of steps after which to save a checkpoint.
save_steps: 100
# Debugging mode flag.
debug:
# Path to a DeepSpeed config file. Left empty as it's not needed for this setup.
deepspeed:

