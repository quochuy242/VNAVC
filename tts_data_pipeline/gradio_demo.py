# app.py

# /// script
# dependencies = [
#   "torch",
#   "transformers",
#   "gradio",
#   "numpy",
#   "soundfile",
#   "torchaudio",
#   "accelerate",
# ]
# ///

import os
import re

import gradio as gr
import numpy as np
import soundfile as sf
import torch
from transformers import AutoModelForSeq2SeqLM, AutoProcessor, AutoTokenizer

# --- Global Configurations ---
MODEL_ID = "qhuy242/spark-tts-finetuned-on-vnavc-dataset"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MAX_NEW_AUDIO_TOKENS = 2048  # Max tokens for audio part
DEFAULT_TEMPERATURE = 0.8
DEFAULT_TOP_K = 50
DEFAULT_TOP_P = 1.0  # Changed to float for consistency


# --- Model and Tokenizer Loading ---
tokenizer = None
model = None
audio_tokenizer = None
sample_rate = 16000  # Default, will be updated from audio_tokenizer config


def load_model_and_tokenizers():
  """Loads the pre-trained model, tokenizer, and audio processor."""
  global tokenizer, model, audio_tokenizer, sample_rate

  print(f"Using device: {DEVICE}")
  print(f"Loading model and tokenizer for {MODEL_ID}...")

  try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID).to(DEVICE)
    audio_tokenizer = AutoProcessor.from_pretrained(MODEL_ID)

    # Enable native 2x faster inference
    if hasattr(model, "for_inference"):
      model.for_inference()
    else:
      print(
        "Warning: 'for_inference' method not found on the model. Skipping native inference speedup."
      )

    sample_rate = audio_tokenizer.config.get("sample_rate", 16000)
    print("Model and tokenizers loaded successfully.")
  except Exception as e:
    print(f"Error loading model components: {e}")
    # Optionally re-raise or handle more gracefully for a Gradio app
    raise RuntimeError(f"Failed to load model components: {e}")


# --- Speech Generation Core Logic ---
def generate_speech_from_text(
  text: str,
  temperature: float = DEFAULT_TEMPERATURE,
  top_k: int = DEFAULT_TOP_K,
  top_p: float = DEFAULT_TOP_P,
) -> np.ndarray:
  """
  Generates speech audio from text using the loaded model.

  Args:
      text (str): The text input to be converted to speech.
      temperature (float): Sampling temperature for generation.
      top_k (int): Top-k sampling parameter.
      top_p (float): Top-p (nucleus) sampling parameter.

  Returns:
      np.ndarray: Generated waveform as a NumPy array.
  """
  if not text:
    print("Input text is empty. Returning empty audio.")
    return np.array([], dtype=np.float32)

  if model is None or tokenizer is None or audio_tokenizer is None:
    print(
      "Model components not loaded. Please ensure load_model_and_tokenizers() was called."
    )
    raise RuntimeError("Model components are not loaded.")

  prompt = "".join(
    [
      "<|task_tts|>",
      "<|start_content|>",
      text,
      "<|end_content|>",
      "<|start_global_token|>",
    ]
  )

  model_inputs = tokenizer([prompt], return_tensors="pt").to(DEVICE)

  print("Generating token sequence...")
  generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=MAX_NEW_AUDIO_TOKENS,
    do_sample=True,
    temperature=temperature,
    top_k=top_k,
    top_p=top_p,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id,
  )
  print("Token sequence generated.")

  generated_ids_trimmed = generated_ids[:, model_inputs.input_ids.shape[1] :]
  predicts_text = tokenizer.batch_decode(
    generated_ids_trimmed, skip_special_tokens=False
  )[0]

  semantic_matches = re.findall(r"<\|bicodec_semantic_(\d+)\|>", predicts_text)
  if not semantic_matches:
    print("Warning: No semantic tokens found in the generated output.")
    return np.array([], dtype=np.float32)

  pred_semantic_ids = (
    torch.tensor([int(token) for token in semantic_matches]).long().unsqueeze(0)
  )

  global_matches = re.findall(r"<\|bicodec_global_(\d+)\|>", predicts_text)
  if not global_matches:
    print(
      "Warning: No global tokens found in the generated output (controllable mode). Using default zero global token."
    )
    pred_global_ids = torch.zeros((1, 1), dtype=torch.long)
  else:
    pred_global_ids = (
      torch.tensor([int(token) for token in global_matches]).long().unsqueeze(0)
    )

  pred_global_ids = pred_global_ids.unsqueeze(0)  # Ensure (1, 1, N_global)

  print(f"Found {pred_semantic_ids.shape[1]} semantic tokens.")
  print(f"Found {pred_global_ids.shape[2]} global tokens.")

  print("Detokenizing audio tokens...")
  # Ensure audio_tokenizer and its internal model are on the correct device
  audio_tokenizer.device = DEVICE  # Set device explicitly for the processor
  audio_tokenizer.model.to(DEVICE)  # Move the internal model to device
  with torch.no_grad():
    wav_np = audio_tokenizer.detokenize(
      pred_global_ids.to(DEVICE).squeeze(0),  # Shape (1, N_global)
      pred_semantic_ids.to(DEVICE),  # Shape (1, N_semantic)
    )
  print("Detokenization complete.")

  return wav_np


# --- Gradio Interface Function ---
def gradio_interface_fn(
  text_input: str, temperature: float, top_k: int, top_p: float
) -> tuple[int, np.ndarray]:
  """
  Wrapper function for Gradio interface to generate speech.
  Takes text input and generation parameters, returns sample rate and audio array.
  """
  try:
    generated_waveform = generate_speech_from_text(
      text_input, temperature=temperature, top_k=top_k, top_p=top_p
    )
    if generated_waveform.size > 0:
      return sample_rate, generated_waveform
    else:
      gr.Warning(
        "Kh√¥ng th·ªÉ t·∫°o √¢m thanh. Vui l√≤ng th·ª≠ l·∫°i v·ªõi vƒÉn b·∫£n kh√°c ho·∫∑c ƒëi·ªÅu ch·ªânh tham s·ªë."
      )
      return sample_rate, np.array([], dtype=np.float32)
  except Exception as e:
    gr.Error(f"ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh t·∫°o √¢m thanh: {e}")
    return sample_rate, np.array([], dtype=np.float32)


# --- Gradio UI Definition ---
def create_gradio_interface():
  """Creates and returns the Gradio Interface object."""
  return gr.Interface(
    fn=gradio_interface_fn,
    inputs=[
      gr.Textbox(
        lines=3,
        placeholder="Nh·∫≠p vƒÉn b·∫£n ti·∫øng Vi·ªát ƒë·ªÉ chuy·ªÉn th√†nh gi·ªçng n√≥i...",
        label="VƒÉn b·∫£n ƒë·∫ßu v√†o",
      ),
      gr.Slider(
        minimum=0.1,
        maximum=2.0,
        value=DEFAULT_TEMPERATURE,
        step=0.1,
        label="Nhi·ªát ƒë·ªô (Temperature)",
        info="Ki·ªÉm so√°t t√≠nh ng·∫´u nhi√™n c·ªßa gi·ªçng n√≥i. Gi√° tr·ªã cao h∆°n t·∫°o ra gi·ªçng n√≥i ƒëa d·∫°ng h∆°n.",
      ),
      gr.Slider(
        minimum=1,
        maximum=250,  # A reasonable max for top_k
        value=DEFAULT_TOP_K,
        step=1,
        label="Top-K",
        info="Ch·ªâ xem x√©t K t·ª´ c√≥ kh·∫£ nƒÉng cao nh·∫•t.",
      ),
      gr.Slider(
        minimum=0.1,
        maximum=1.0,
        value=DEFAULT_TOP_P,
        step=0.05,
        label="Top-P (Nucleus Sampling)",
        info="Ch·ªâ xem x√©t c√°c t·ª´ m√† t·ªïng x√°c su·∫•t c·ªßa ch√∫ng ƒë·∫°t P.",
      ),
    ],
    outputs=gr.Audio(
      label="√Çm thanh ƒë∆∞·ª£c t·∫°o",
      type="numpy",
      autoplay=True,
    ),
    title="üåü SparkTTS Ti·∫øng Vi·ªát: Chuy·ªÉn VƒÉn B·∫£n Th√†nh Gi·ªçng N√≥i üåü",
    description=(
      "S·ª≠ d·ª•ng m√¥ h√¨nh SparkTTS ƒë√£ tinh ch·ªânh tr√™n t·∫≠p d·ªØ li·ªáu VNAVC ƒë·ªÉ t·∫°o gi·ªçng n√≥i t·ª´ vƒÉn b·∫£n ti·∫øng Vi·ªát. "
      "B·∫°n c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh c√°c tham s·ªë t·∫°o ƒë·ªÉ ki·ªÉm so√°t k·∫øt qu·∫£."
    ),
    examples=[
      [
        "Xin ch√†o, t√¥i l√† m√¥ h√¨nh chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh gi·ªçng n√≥i.",
        DEFAULT_TEMPERATURE,
        DEFAULT_TOP_K,
        DEFAULT_TOP_P,
      ],
      [
        "C·∫£m ∆°n b·∫°n ƒë√£ l·∫Øng nghe. Hy v·ªçng b·∫°n th√≠ch demo n√†y.",
        0.6,
        50,
        0.95,
      ],  # V√≠ d·ª• v·ªõi tham s·ªë kh√°c
      [
        "H·ªì Ch√≠ Minh l√† th√†nh ph·ªë l·ªõn nh·∫•t Vi·ªát Nam.",
        DEFAULT_TEMPERATURE,
        DEFAULT_TOP_K,
        DEFAULT_TOP_P,
      ],
    ],
    allow_flagging="never",
    css="footer {visibility: hidden}",  # Hide Gradio footer for a cleaner look
  )


# --- Main Execution Block ---
if __name__ == "__main__":
  load_model_and_tokenizers()
  iface = create_gradio_interface()
  print("Launching Gradio interface...")
  iface.queue().launch(share=True)
